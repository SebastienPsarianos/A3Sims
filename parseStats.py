# NOTE:
# By Kevin Chen
# This script employed GPT-5 generated code. It generates the visualization based on the statistics files
# (Graphing script generated by ChatGPT-5 from my previous Tutorial 3 script, OpenAI, personal communication, October 10, 2025)
# And adapted for A2!
# This was modified for A3, again, to parse the results for Part 3

#!/usr/bin/env python3
import argparse
import csv
import itertools
import json
from pathlib import Path
import re
from typing import Dict, Iterable, List, Optional, Tuple, Any, Callable
from collections import defaultdict

# -----------------------------
# Constants and configuration
# -----------------------------

END_MARKER_PREFIX = "---------- End Simulation Statistics"
DEFAULT_SESSION_INDEX = 1

DEFAULT_CPU_PARAMETERS = {
    "--commit_width": "1",
    "--decode_width": "1",
    "--dispatch_width": "1",
    "--fetch_buffer_size": "64",
    "--fetch_queue_size": "4",
    "--fetch_width": "1",
    "--fu_pool": "basic",
    "--issue_width": "1",
    "--lq_entries": "4",
    "--num_iq_entries": "16",
    "--num_rob_entries": "32",
    "--rename_width": "1",
    "--sq_entries": "4",
}
# Ensure alphabetical order for filename decoding
PARAM_KEYS_SORTED = sorted(DEFAULT_CPU_PARAMETERS.keys())
FILENAME_RE = re.compile(r"^(?P<workload>[A-Za-z0-9_]+)(?:-(?P<vals>.*))?$")

# -----------------------------
# IO utilities
# -----------------------------


def walk_text_files(root: Path) -> Iterable[Path]:
    if root.is_file() and root.suffix.lower() == ".txt":
        yield root
        return
    for p in root.rglob("*.txt"):
        if p.is_file():
            yield p

# -----------------------------
# Session parsing
# -----------------------------


def load_sessions(text_file: Path) -> List[List[str]]:
    with open(text_file, "r", encoding="utf-8", errors="ignore") as f:
        sessions: List[List[str]] = []
        for key, subiter in itertools.groupby(
            f, lambda line: line.startswith(END_MARKER_PREFIX)
        ):
            if key:
                continue
            lines = [line.rstrip("\n") for line in subiter]
            if lines:
                sessions.append(lines)
    return sessions


def split_and_index(lines: List[str]) -> Dict[str, Tuple[str, ...]]:
    mapping: Dict[str, Tuple[str, ...]] = {}
    for string in lines:
        parts = [p.strip() for p in re.split(r"\s{2,}", string) if p.strip()]
        if parts:
            mapping[parts[0]] = tuple(parts[1:]) if len(parts) > 1 else tuple()
    return mapping

# -----------------------------
# Helpers and extractors
# -----------------------------


def safe_float_from_mapping(mapping: Dict[str, Tuple[str, ...]], key: str) -> Optional[float]:
    vals = mapping.get(key, (None,))
    try:
        return float(vals[0]) if vals and vals[0] is not None else None
    except Exception:
        return None


def safe_int_from_mapping(mapping: Dict[str, Tuple[str, ...]], key: str) -> Optional[int]:
    vals = mapping.get(key, (None,))
    try:
        return int(float(vals[0])) if vals and vals[0] is not None else None
    except Exception:
        return None


def as_float(val: Any) -> Optional[float]:
    try:
        return float(val)
    except Exception:
        return None


Extractor = Callable[[Dict[str, Tuple[str, ...]]], Optional[float]]


def extract_ipc(mapping: Dict[str, Tuple[str, ...]]) -> Optional[float]:
    return safe_float_from_mapping(mapping, "system.cpu.ipc")


def extract_issue_rate(mapping: Dict[str, Tuple[str, ...]]) -> Optional[float]:
    return safe_float_from_mapping(mapping, "system.cpu.issueRate")


def extract_total_insts(mapping: Dict[str, Tuple[str, ...]]) -> Optional[float]:
    return safe_float_from_mapping(mapping, "system.cpu.commitStats0.numInsts")


def extract_demand_misses(mapping: Dict[str, Tuple[str, ...]]) -> Optional[float]:
    return safe_float_from_mapping(mapping, "system.cpu.l1d.demandMisses::total")


def extract_demand_accesses(mapping: Dict[str, Tuple[str, ...]]) -> Optional[float]:
    return safe_float_from_mapping(mapping, "system.cpu.l1d.demandAccesses::total")


SCALAR_EXTRACTORS: Dict[str, Extractor] = {
    "ipc": extract_ipc,
    "issue_rate": extract_issue_rate,
    "total_insts": extract_total_insts,
    "demand_misses": extract_demand_misses,
    "demand_accesses": extract_demand_accesses,
}


def derive_metrics(row: Dict[str, Any]) -> None:
    total_insts = as_float(row.get("total_insts"))
    demand_misses = as_float(row.get("demand_misses"))
    demand_accesses = as_float(row.get("demand_accesses"))

    miss_rate = None
    misses_per_instruction = None
    misses_per_kilo_instruction = None
    memory_access_per_instruction = None

    if demand_misses is not None and demand_accesses not in (None, 0):
        miss_rate = demand_misses / demand_accesses

    if demand_misses is not None and total_insts not in (None, 0):
        misses_per_instruction = demand_misses / total_insts
        misses_per_kilo_instruction = misses_per_instruction * 1000.0

    if demand_accesses is not None and total_insts not in (None, 0):
        memory_access_per_instruction = demand_accesses / total_insts

    row["miss_rate"] = miss_rate
    row["misses_per_instruction"] = misses_per_instruction
    row["misses_per_kilo_instruction"] = misses_per_kilo_instruction
    row["memory_access_per_instruction"] = memory_access_per_instruction

# New: extended derived metrics with category-prefixed names


def derive_metrics_plus(row: Dict[str, Any], mapping: Dict[str, Tuple[str, ...]]) -> Dict[str, Any]:
    def g(k): return safe_float_from_mapping(mapping, k)

    total_insts = as_float(row.get("total_insts")) or 0.0
    num_cycles = g("system.cpu.numCycles") or 0.0
    num_ops = g("system.cpu.commitStats0.numOps") or 0.0
    committed_branches = g("system.cpu.branchPred.committed_0::total") or 0.0

    def div(a, b):
        try:
            return (a if a is not None else None) if (b is None or float(b) == 0.0) else float(a) / float(b)
        except Exception:
            return None

    metrics_extra: Dict[str, Optional[float]] = {}

    # ALU/FP/SIMD
    int_alu = g("system.cpu.intAluAccesses")
    fp_alu = g("system.cpu.fpAluAccesses")
    metrics_extra.update({
        "ALUFP.int_alu_per_inst": div(int_alu, total_insts),
        "ALUFP.fp_alu_per_inst": div(fp_alu, total_insts),
        "ALUFP.int_fp_ratio": div(int_alu, fp_alu if (fp_alu and fp_alu != 0) else None),
        "ALUFP.producer_consumer_ratio": div(g("system.cpu.iew.producerInst"), g("system.cpu.iew.consumerInst")),
        "ALUFP.wb_rate": g("system.cpu.iew.wbRate"),
        "ALUFP.wb_fanout": g("system.cpu.iew.wbFanout"),
        "ALUFP.simd_frac_committed": div(
            sum(filter(None, [
                g("system.cpu.commit.committedInstType_0::SimdAlu"),
                g("system.cpu.commit.committedInstType_0::SimdCvt"),
                g("system.cpu.commit.committedInstType_0::SimdMisc"),
                g("system.cpu.commit.committedInstType_0::SimdDiv"),
                g("system.cpu.commit.committedInstType_0::SimdSqrt"),
                g("system.cpu.commit.committedInstType_0::SimdFloatAdd"),
                g("system.cpu.commit.committedInstType_0::SimdFloatAlu"),
                g("system.cpu.commit.committedInstType_0::SimdFloatCmp"),
                g("system.cpu.commit.committedInstType_0::SimdFloatCvt"),
                g("system.cpu.commit.committedInstType_0::SimdFloatDiv"),
                g("system.cpu.commit.committedInstType_0::SimdFloatMisc"),
                g("system.cpu.commit.committedInstType_0::SimdFloatMult"),
                g("system.cpu.commit.committedInstType_0::SimdFloatMultAcc"),
                g("system.cpu.commit.committedInstType_0::SimdFloatMatMultAcc"),
                g("system.cpu.commit.committedInstType_0::SimdFloatSqrt"),
                g("system.cpu.commit.committedInstType_0::SimdReduceAdd"),
                g("system.cpu.commit.committedInstType_0::SimdReduceAlu"),
                g("system.cpu.commit.committedInstType_0::SimdReduceCmp"),
                g("system.cpu.commit.committedInstType_0::SimdFloatReduceAdd"),
                g("system.cpu.commit.committedInstType_0::SimdFloatReduceCmp"),
                g("system.cpu.commit.committedInstType_0::SimdAes"),
                g("system.cpu.commit.committedInstType_0::SimdAesMix"),
                g("system.cpu.commit.committedInstType_0::SimdSha1Hash"),
                g("system.cpu.commit.committedInstType_0::SimdSha1Hash2"),
                g("system.cpu.commit.committedInstType_0::SimdSha256Hash"),
                g("system.cpu.commit.committedInstType_0::SimdSha256Hash2"),
                g("system.cpu.commit.committedInstType_0::SimdShaSigma2"),
                g("system.cpu.commit.committedInstType_0::SimdShaSigma3"),
                g("system.cpu.commit.committedInstType_0::SimdPredAlu"),
                g("system.cpu.commit.committedInstType_0::SimdUnitStrideLoad"),
                g("system.cpu.commit.committedInstType_0::SimdUnitStrideStore"),
                g("system.cpu.commit.committedInstType_0::SimdUnitStrideMaskLoad"),
                g("system.cpu.commit.committedInstType_0::SimdUnitStrideMaskStore"),
                g("system.cpu.commit.committedInstType_0::SimdStridedLoad"),
                g("system.cpu.commit.committedInstType_0::SimdStridedStore"),
                g("system.cpu.commit.committedInstType_0::SimdIndexedLoad"),
                g("system.cpu.commit.committedInstType_0::SimdIndexedStore"),
                g("system.cpu.commit.committedInstType_0::SimdWholeRegisterLoad"),
                g("system.cpu.commit.committedInstType_0::SimdWholeRegisterStore"),
                g("system.cpu.commit.committedInstType_0::SimdUnitStrideFaultOnlyFirstLoad"),
                g("system.cpu.commit.committedInstType_0::SimdUnitStrideSegmentedLoad"),
                g("system.cpu.commit.committedInstType_0::SimdUnitStrideSegmentedStore"),
                g("system.cpu.commit.committedInstType_0::SimdUnitStrideSegmentedFaultOnlyFirstLoad"),
                g("system.cpu.commit.committedInstType_0::SimdStrideSegmentedLoad"),
                g("system.cpu.commit.committedInstType_0::SimdStrideSegmentedStore"),
            ])),
            num_ops if num_ops else None
        ),
        "ALUFP.int_alu_commit_frac": div(g("system.cpu.commit.committedInstType_0::IntAlu"), num_ops),
        "ALUFP.int_mult_commit_frac": div(g("system.cpu.commit.committedInstType_0::IntMult"), num_ops),
        "ALUFP.int_div_commit_frac": div(g("system.cpu.commit.committedInstType_0::IntDiv"), num_ops),
        "ALUFP.fp_add_commit_frac": div(g("system.cpu.commit.committedInstType_0::FloatAdd"), num_ops),
        "ALUFP.fp_mult_commit_frac": div(g("system.cpu.commit.committedInstType_0::FloatMult"), num_ops),
        "ALUFP.fp_div_commit_frac": div(g("system.cpu.commit.committedInstType_0::FloatDiv"), num_ops),
        "ALUFP.memread_commit_frac": div(g("system.cpu.commit.committedInstType_0::MemRead"), num_ops),
        "ALUFP.memwrite_commit_frac": div(g("system.cpu.commit.committedInstType_0::MemWrite"), num_ops),
    })

    # Frontend
    metrics_extra.update({
        "Frontend.fetch_rate": g("system.cpu.fetchStats0.fetchRate"),
        "Frontend.fetch_branch_rate": g("system.cpu.fetchStats0.branchRate"),
        "Frontend.fetch_block_stall_frac": div(g("system.cpu.fetchStats0.icacheStallCycles"), g("system.cpu.fetch.cycles")),
        "Frontend.fetch_util_mean": g("system.cpu.fetch.nisnDist::mean"),
        "Frontend.decode_idle_frac": div(g("system.cpu.decode.idleCycles"), num_cycles),
        "Frontend.decode_blocked_frac": div(g("system.cpu.decode.blockedCycles"), num_cycles),
        "Frontend.rename_idle_frac": div(g("system.cpu.rename.idleCycles"), num_cycles),
        "Frontend.rename_blocked_frac": div(g("system.cpu.rename.blockCycles"), num_cycles),
        "Frontend.rename_rob_full_events": g("system.cpu.rename.ROBFullEvents"),
        "Frontend.rename_iq_full_events": g("system.cpu.rename.IQFullEvents"),
        "Frontend.rename_lq_full_events": g("system.cpu.rename.LQFullEvents"),
        "Frontend.rename_sq_full_events": g("system.cpu.rename.SQFullEvents"),
        "Frontend.rename_serialize_stall_frac": div(g("system.cpu.rename.serializeStallCycles"), num_cycles),
    })

    # Backend / LSQ
    metrics_extra.update({
        "Backend.issue_rate": row.get("issue_rate"),
        "Backend.execute_inst_rate": g("system.cpu.executeStats0.instRate"),
        "Backend.iew_block_frac": div(g("system.cpu.iew.blockCycles"), num_cycles),
        "Backend.iew_mispredict_events": g("system.cpu.iew.branchMispredicts"),
        "Backend.lsq_full_events": g("system.cpu.iew.lsqFullEvents"),
        "Backend.iq_full_events": g("system.cpu.iew.iqFullEvents"),
        "Backend.load_to_use_mean": g("system.cpu.lsq0.loadToUse::mean"),
        "Backend.mem_refs_per_inst": div(g("system.cpu.executeStats0.numMemRefs"), total_insts),
        "Backend.load_frac_committed": div(g("system.cpu.commitStats0.numLoadInsts"), total_insts),
        "Backend.store_frac_committed": div(g("system.cpu.commitStats0.numStoreInsts"), total_insts),
    })

    # Branch
    branch_misp_total = g("system.cpu.branchPred.mispredicted_0::total")
    btb_miss_total = g("system.cpu.branchPred.mispredictDueToBTBMiss_0::total")
    pred_miss_total = g(
        "system.cpu.branchPred.mispredictDueToPredictor_0::total")
    metrics_extra.update({
        "Branch.committed_branches": committed_branches,
        "Branch.branch_mispred_total": branch_misp_total,
        "Branch.branch_mispred_rate": div(branch_misp_total, committed_branches),
        "Branch.mispred_due_predictor_rate": div(pred_miss_total, committed_branches),
        "Branch.mispred_due_btb_rate": div(btb_miss_total, committed_branches),
        "Branch.btb_hit_ratio": g("system.cpu.branchPred.BTBHitRatio"),
        "Branch.target_wrong_rate": div(g("system.cpu.branchPred.targetWrong_0::total"), g("system.cpu.branchPred.BTBLookups")),
        "Branch.ras_correct_rate": div(g("system.cpu.branchPred.ras.correct"), g("system.cpu.branchPred.ras.used")),
        "Branch.mispredicts_per_ki": div(1000.0 * (g("system.cpu.iew.branchMispredicts") or 0.0), total_insts if total_insts else None),
    })

    # ROB / Commit
    metrics_extra.update({
        "ROBCommit.commit_bw_util_mean": g("system.cpu.commit.numCommittedDist::mean"),
        "ROBCommit.commit_squashed_per_committed": div(g("system.cpu.commit.commitSquashedInsts"), g("system.cpu.commitStats0.numInsts")),
        "ROBCommit.commit_non_spec_stall_rate": div(g("system.cpu.commit.commitNonSpecStalls"), num_cycles),
        "ROBCommit.rob_reads_per_inst": div(g("system.cpu.rob.reads"), total_insts),
        "ROBCommit.rob_writes_per_inst": div(g("system.cpu.rob.writes"), total_insts),
    })

    # Cache / L1 / Memory
    metrics_extra.update({
        "Cache.l1d_demand_miss_rate": g("system.cpu.l1d.demandMissRate::total"),
        "Cache.l1d_read_miss_rate": g("system.cpu.l1d.ReadReq.missRate::total"),
        "Cache.l1d_write_miss_rate": g("system.cpu.l1d.WriteReq.missRate::total"),
        "Cache.l1d_mshr_miss_rate": g("system.cpu.l1d.demandMshrMissRate::total"),
        "Cache.l1d_avg_miss_lat_ticks": g("system.cpu.l1d.demandAvgMissLatency::total"),
        "Cache.l1d_avg_mshr_miss_lat_ticks": g("system.cpu.l1d.demandAvgMshrMissLatency::total"),
        "Cache.l1i_miss_rate": g("system.cpu.l1i.demandMissRate::total"),
        "Cache.l1i_avg_miss_lat_ticks": g("system.cpu.l1i.demandAvgMissLatency::total"),
        "Cache.l1i_mshr_miss_rate": g("system.cpu.l1i.demandMshrMissRate::total"),
        "Cache.l1d_replacements": g("system.cpu.l1d.replacements"),
        "Cache.l1i_replacements": g("system.cpu.l1i.replacements"),
        "Memory.bytes_read_inst": g("system.mem_ctrl.requestorReadBytes::cpu.inst"),
        "Memory.bytes_read_data": g("system.mem_ctrl.requestorReadBytes::cpu.data"),
        "Memory.mem_read_lat_inst_ticks": g("system.mem_ctrl.requestorReadAvgLat::cpu.inst"),
        "Memory.mem_read_lat_data_ticks": g("system.mem_ctrl.requestorReadAvgLat::cpu.data"),
        "Memory.dram_read_bw_total_Bps": g("system.mem_ctrl.dram.bwRead::total"),
        "Memory.dram_bus_util_percent": g("system.mem_ctrl.dram.busUtil"),
        "Memory.mem_read_to_write_ratio": div(g("system.mem_ctrl.readReqs"), g("system.mem_ctrl.writeReqs")),
        "Memory.mem_avg_rd_queue_len": g("system.mem_ctrl.avgRdQLen"),
        "Memory.mem_avg_wr_queue_len": g("system.mem_ctrl.avgWrQLen"),
        "Memory.mem_rd_turnarounds": g("system.mem_ctrl.numReadWriteTurnArounds"),
        "Memory.mem_wr_turnarounds": g("system.mem_ctrl.numWriteReadTurnArounds"),
    })

    # Throughput / Sanity
    metrics_extra.update({
        "Throughput.cpi_core": g("system.cpu.cpi"),
        "Throughput.ipc_core": g("system.cpu.ipc"),
        "Throughput.cycles": num_cycles,
        "Throughput.sim_seconds": g("simSeconds"),
    })

    # Attach into row so save_consolidated_* can also use if needed
    row["derived_metrics"] = metrics_extra
    return metrics_extra

# -----------------------------
# Filename parsing
# -----------------------------


def parse_workload_and_config_from_filename(file: Path) -> Tuple[str, Dict[str, str], List[str]]:
    m = FILENAME_RE.match(file.stem)
    workload = "unknown"
    values_part = ""
    if m:
        workload = m.group("workload") or "unknown"
        values_part = m.group("vals") or ""

    raw_values = [] if not values_part else values_part.split("-")

    cfg: Dict[str, str] = {}
    if raw_values:
        if len(raw_values) != len(PARAM_KEYS_SORTED):
            print(f"[warn] {file.name}: expected {
                  len(PARAM_KEYS_SORTED)} config tokens, got {len(raw_values)}")
        for i, key in enumerate(PARAM_KEYS_SORTED):
            val = raw_values[i] if i < len(raw_values) else ""
            cfg[key] = val
    else:
        cfg = dict(DEFAULT_CPU_PARAMETERS)
    return workload, cfg, raw_values

# -----------------------------
# Core processing
# -----------------------------


def select_session(sessions: List[List[str]], session_index: int) -> Optional[List[str]]:
    if not sessions:
        return None
    if session_index < 0 or session_index >= len(sessions):
        return None
    return sessions[session_index]


def process_file(file: Path, session_index: int) -> Optional[Dict[str, Any]]:
    try:
        sessions = load_sessions(file)
        if not sessions:
            print(f"[warn] {file} contains no sessions")
            return None

        sel = select_session(sessions, session_index)
        if sel is None:
            print(f"[warn] {file} missing session index {
                  session_index}, has {len(sessions)} sessions")
            return None

        mapping = split_and_index(sel)
        workload, cfg_from_name, raw_vals = parse_workload_and_config_from_filename(
            file)

        row: Dict[str, Any] = {
            "file": str(file),
            "filename": file.name,
            "dir": str(file.parent),
            "workload": workload,
            "config_from_name": cfg_from_name,
            "config_tokens_raw": raw_vals,
            "param_keys_sorted": PARAM_KEYS_SORTED,
        }

        for key, extractor in SCALAR_EXTRACTORS.items():
            row[key] = extractor(mapping)

        derive_metrics(row)
        # New: extended derived metrics
        derived_extra = derive_metrics_plus(row, mapping)
        row["raw_mapping"] = mapping
        row["_derived_metrics_keys"] = sorted(derived_extra.keys())
        return row

    except Exception as e:
        print(f"[error] Exception while processing {file}: {e}")
        return None


def process_root_directory(root_dir: Path, session_index: int) -> List[Dict[str, Any]]:
    print(f"Examining root directory {root_dir}")
    results: List[Dict[str, Any]] = []
    for file in walk_text_files(root_dir):
        row = process_file(file, session_index=session_index)
        if row is not None:
            results.append(row)
    return results

# -----------------------------
# Saving: per-file JSON + optional consolidated
# -----------------------------


def save_per_file_jsons(results: List[Dict[str, Any]], root_dir: Path, out_dir: Path, include_raw: bool = True) -> List[Path]:
    out_paths: List[Path] = []
    for r in results:
        src_path = Path(r["file"])
        rel = src_path.relative_to(root_dir)
        out_path = out_dir / "per_file" / rel.with_suffix(".json")
        out_path.parent.mkdir(parents=True, exist_ok=True)

        payload = {
            "configuration_description": "Filename encodes CPU parameter values in alphabetical order of parameter names.",
            "defaultCpuParameters_sorted_keys": PARAM_KEYS_SORTED,
            "defaultCpuParameters": DEFAULT_CPU_PARAMETERS,
            "workload": r.get("workload"),
            "file": r.get("file"),
            "filename": r.get("filename"),
            "dir": r.get("dir"),
            "config_from_name": r.get("config_from_name"),
            "config_tokens_raw": r.get("config_tokens_raw"),
            "metrics": {
                # Original shortlist + classic derived
                "ipc": r.get("ipc"),
                "issue_rate": r.get("issue_rate"),
                "total_insts": r.get("total_insts"),
                "demand_misses": r.get("demand_misses"),
                "demand_accesses": r.get("demand_accesses"),
                "miss_rate": r.get("miss_rate"),
                "misses_per_instruction": r.get("misses_per_instruction"),
                "misses_per_kilo_instruction": r.get("misses_per_kilo_instruction"),
                "memory_access_per_instruction": r.get("memory_access_per_instruction"),
                # New derived metrics, flattened into metrics with category prefixes
                **(r.get("derived_metrics") or {}),
            },
        }
        if include_raw:
            raw = r.get("raw_mapping", {})
            if isinstance(raw, dict):
                payload["raw_mapping"] = {k: list(v) for k, v in raw.items()}

        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(payload, f, indent=2)
        out_paths.append(out_path)
    print(f"Saved {len(out_paths)} per-file JSONs under {out_dir/'per_file'}")
    return out_paths


def save_all_extracted_stats_per_workload(results: List[Dict[str, Any]], out_dir: Path) -> List[Path]:
    out_dir.mkdir(parents=True, exist_ok=True)
    by_workload: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    for r in results:
        by_workload[r.get("workload", "unknown")].append(r)

    out_paths: List[Path] = []
    for workload, rows in by_workload.items():
        rows_sorted = sorted(rows, key=lambda x: x.get("filename", ""))
        out_path = out_dir / f"all_extracted_stats_{workload}.txt"
        with open(out_path, "w", encoding="utf-8") as f:
            for r in rows_sorted:
                mapping: Dict[str, Tuple[str, ...]] = r.get(
                    "raw_mapping", {}) or {}
                f.write(f"===== {r.get('filename', '')} =====\n")
                f.write(f"dir: {r.get('dir', '')}\n")
                f.write(f"workload: {r.get('workload', 'unknown')}\n")
                f.write("---- parsed config (from filename) ----\n")
                cfg = r.get("config_from_name", {})
                for k in PARAM_KEYS_SORTED:
                    f.write(f"{k}\t{cfg.get(k, '')}\n")
                f.write("---- stats ----\n")
                for k in sorted(mapping.keys()):
                    vals = mapping[k]
                    if isinstance(vals, (list, tuple)):
                        line = "\t".join([k] + [str(v) for v in vals])
                    else:
                        line = f"{k}\t{vals}"
                    f.write(line + "\n")
                f.write("\n")
        print(f"Saved all extracted stats for {workload} to {out_path}")
        out_paths.append(out_path)
    return out_paths


def save_consolidated_json(results: List[Dict[str, Any]], out_path: Path, include_raw: bool = True) -> Path:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    payload = []
    for r in results:
        row = dict(r)
        if not include_raw:
            row.pop("raw_mapping", None)
        else:
            if "raw_mapping" in row and isinstance(row["raw_mapping"], dict):
                row["raw_mapping"] = {k: list(v)
                                      for k, v in row["raw_mapping"].items()}
        payload.append(row)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)
    print(f"Saved consolidated JSON to {out_path}")
    return out_path


def save_consolidated_csv(results: List[Dict[str, Any]], out_path: Path) -> Path:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    # Collect dynamic derived keys to include in CSV
    derived_keys = set()
    for r in results:
        for k in (r.get("derived_metrics") or {}).keys():
            derived_keys.add(k)
    derived_keys = sorted(derived_keys)

    fields = [
        "file",
        "filename",
        "dir",
        "workload",
        *PARAM_KEYS_SORTED,
        "ipc",
        "issue_rate",
        "total_insts",
        "demand_misses",
        "demand_accesses",
        "miss_rate",
        "misses_per_instruction",
        "misses_per_kilo_instruction",
        "memory_access_per_instruction",
        *derived_keys,
    ]
    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fields)
        writer.writeheader()
        for r in results:
            row = {k: r.get(k)
                   for k in ["file", "filename", "dir", "workload"]}
            cfg = r.get("config_from_name", {}) or {}
            for k in PARAM_KEYS_SORTED:
                row[k] = cfg.get(k)
            row.update({
                "ipc": r.get("ipc"),
                "issue_rate": r.get("issue_rate"),
                "total_insts": r.get("total_insts"),
                "demand_misses": r.get("demand_misses"),
                "demand_accesses": r.get("demand_accesses"),
                "miss_rate": r.get("miss_rate"),
                "misses_per_instruction": r.get("misses_per_instruction"),
                "misses_per_kilo_instruction": r.get("misses_per_kilo_instruction"),
                "memory_access_per_instruction": r.get("memory_access_per_instruction"),
            })
            # Add derived metrics
            for k in derived_keys:
                row[k] = (r.get("derived_metrics") or {}).get(k)
            writer.writerow(row)
    print(f"Saved consolidated CSV to {out_path}")
    return out_path

# -----------------------------
# Visualization stub (optional)
# -----------------------------


def visualize_stub(results: List[Dict[str, Any]], out_dir: Path) -> None:
    try:
        import matplotlib.pyplot as plt
    except Exception as e:
        print(f"[warn] matplotlib not available: {e}")
        return

    out_dir.mkdir(parents=True, exist_ok=True)
    by_wl: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    for r in results:
        by_wl[r.get("workload", "unknown")].append(r)

    any_point = any((r.get("miss_rate") is not None and r.get(
        "ipc") is not None) for r in results)
    if not any_point:
        print("[info] No data points for visualization.")
        return

    plt.figure(figsize=(8, 5))
    for wl, rows in by_wl.items():
        xs, ys = [], []
        for r in rows:
            if r.get("miss_rate") is not None and r.get("ipc") is not None:
                xs.append(r["miss_rate"])
                ys.append(r["ipc"])
        if xs and ys:
            plt.scatter(xs, ys, label=wl, alpha=0.7)

    plt.xlabel("L1D miss rate")
    plt.ylabel("IPC")
    plt.title("IPC vs. L1D miss rate by workload")
    plt.legend(loc="best", fontsize="small")
    plt.grid(True, alpha=0.3)
    out_path = out_dir / "ipc_vs_missrate.png"
    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close()
    print(f"Saved visualization stub plot to {out_path}")

# -----------------------------
# CLI
# -----------------------------


def main():
    parser = argparse.ArgumentParser(
        description="Aggregate gem5 statistics; produce per-file JSONs with config parsed from filenames.")
    parser.add_argument(
        "root", type=Path, help="Root directory containing gem5 stats .txt files.")
    parser.add_argument("--session-index", type=int, default=DEFAULT_SESSION_INDEX,
                        help=f"Session index to analyze (default: {DEFAULT_SESSION_INDEX}).")
    parser.add_argument("--out-dir", type=Path, default=Path("out_stats"),
                        help="Output directory for results.")
    parser.add_argument("--no-raw", action="store_true",
                        help="Do not include raw_mapping in JSON outputs.")
    parser.add_argument("--no-visual", action="store_true",
                        help="Skip visualization.")
    parser.add_argument("--no-consolidated", action="store_true",
                        help="Do not write consolidated JSON/CSV or per-workload text.")
    args = parser.parse_args()

    results = process_root_directory(
        args.root, session_index=args.session_index)
    if not results:
        print("No results extracted. Exiting.")
        return

    # Always write per-file JSONs
    save_per_file_jsons(results, root_dir=args.root,
                        out_dir=args.out_dir, include_raw=not args.no_raw)

    # Optional aggregate outputs
    if not args.no_consolidated:
        save_all_extracted_stats_per_workload(results, args.out_dir)
        save_consolidated_json(results, args.out_dir /
                               "consolidated.json", include_raw=not args.no_raw)
        save_consolidated_csv(results, args.out_dir / "consolidated.csv")

    if not args.no_visual:
        visualize_stub(results, args.out_dir)


if __name__ == "__main__":
    main()
